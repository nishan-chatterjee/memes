{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 13\n",
      "Example\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '013', 'text': \"ELI5: What's quiet quitting?\", 'source': 'Reddit'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"data/corpus/dummy_corpus.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"Number of documents: {len(dataset)}\")\n",
    "print(\"Example\")\n",
    "dataset[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '013',\n",
       " 'text': \"ELI5: What's quiet quitting?\",\n",
       " 'sentence n-gram pairs': [[\"ELI5: What's quiet quitting?\",\n",
       "   [\"ELI5: What's\",\n",
       "    \"What's quiet\",\n",
       "    'quiet quitting?',\n",
       "    \"ELI5: What's quiet\",\n",
       "    \"What's quiet quitting?\",\n",
       "    \"ELI5: What's quiet quitting?\"]]],\n",
       " 'source': 'Reddit'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "\"\"\"\n",
    "Extracts n-grams from the text of each entry in the dataset with the following format:\n",
    "{\n",
    "    \"id\": \"some_id\",\n",
    "    \"text\": \"some text\",\n",
    "    \"sentence n-gram pairs\": [\n",
    "        [\"sentence1\", [\"ngram1\", \"ngram2\", ...]],\n",
    "        [\"sentence2\", [\"ngram3\", \"ngram4\", ...]],\n",
    "        ...\n",
    "    ],\n",
    "    \"source\": \"some source\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def n_grams(dataset, n_low=2, n_high=5):\n",
    "    def extract_ngrams(sentence, n_low, n_high):\n",
    "        words = sentence.split()  # Split the sentence into words\n",
    "        ngram_list = []\n",
    "        for n in range(n_low, n_high + 1):\n",
    "            ngrams = list(itertools.islice(zip(*[words[i:] for i in range(n)]), 0, None))\n",
    "            ngram_list.extend([\" \".join(ngram) for ngram in ngrams])\n",
    "        return ngram_list\n",
    "\n",
    "    processed_data = []\n",
    "\n",
    "    for entry in dataset:\n",
    "        text = entry[\"text\"]\n",
    "        sentences = [text]  # Assuming each \"text\" field corresponds to a single sentence\n",
    "        sentence_ngram_pairs = [\n",
    "            [sentence, extract_ngrams(sentence, n_low, n_high)]\n",
    "            for sentence in sentences\n",
    "        ]\n",
    "\n",
    "        processed_entry = {\n",
    "            \"id\": entry[\"id\"],\n",
    "            \"text\": entry[\"text\"],\n",
    "            \"sentence n-gram pairs\": sentence_ngram_pairs,\n",
    "            \"source\": entry[\"source\"],\n",
    "        }\n",
    "        processed_data.append(processed_entry)\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "# Example usage:\n",
    "extracted_n_grams = n_grams(dataset, n_low=2, n_high=5)\n",
    "extracted_n_grams[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique n-grams: 827\n",
      "Example of the n-gram dictionary:\n",
      "'quiet quitting':\n",
      "\t {'count': 4, 'documents': ['002', '003', '004', '007'], 'sentences': ['I never realized I was quiet quitting until someone explained it. It’s crazy how normalized overworking has become.', 'Is quiet quitting really such a bad thing? Doing your job without overextending yourself seems pretty reasonable to me.', 'All this debate about quiet quitting makes me wonder: why is it revolutionary to just do what you’re paid to do?', 'The rise of quiet quitting is just another sign that people are finally valuing their time and energy. It’s about time.']}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Given the list of extracted n-grams, create a dictionary with the following structure:\n",
    "{\n",
    "    \"n-gram\": {\n",
    "        \"count\": <number of occurrences>,\n",
    "        \"documents\": [<list of document IDs where the n-gram occurs>]\n",
    "        \"sentences\": [<list of sentences where the n-gram occurs>]\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "def ngram_occurrences(extracted_n_grams):\n",
    "    ngram_dict = {}\n",
    "\n",
    "    for entry in extracted_n_grams:\n",
    "        for _, ngrams in entry[\"sentence n-gram pairs\"]:\n",
    "            for ngram in ngrams:\n",
    "                if ngram not in ngram_dict:\n",
    "                    ngram_dict[ngram] = {\n",
    "                        \"count\": 1,\n",
    "                        \"documents\": [entry[\"id\"]],\n",
    "                        \"sentences\": [entry[\"text\"]],\n",
    "                    }\n",
    "                else:\n",
    "                    ngram_dict[ngram][\"count\"] += 1\n",
    "                    ngram_dict[ngram][\"documents\"].append(entry[\"id\"])\n",
    "                    ngram_dict[ngram][\"sentences\"].append(entry[\"text\"])\n",
    "\n",
    "    return ngram_dict\n",
    "\n",
    "# Example usage:\n",
    "ngram_dict = ngram_occurrences(extracted_n_grams)\n",
    "print(f\"Number of unique n-grams: {len(ngram_dict)}\")\n",
    "print(\"Example of the n-gram dictionary:\")\n",
    "# sample the most frequent n-gram and print the key-value pair\n",
    "most_frequent = max(ngram_dict, key=lambda x: ngram_dict[x][\"count\"])\n",
    "print(f\"'{most_frequent}':\\n\\t {ngram_dict[most_frequent]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PMI(this is) = 2.367570760443075\n",
      "PMI(corpus is only) = 6.802956905113721\n",
      "PMI(text processing) = 4.3675707604430745\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "def compute_pmi(corpus, external_ngrams, n):\n",
    "    # Step 1: Count n-grams and 1-grams\n",
    "    unigram_counts = defaultdict(int)\n",
    "    ngram_counts = defaultdict(int)\n",
    "    total_unigrams = 0\n",
    "    total_ngrams = 0\n",
    "    \n",
    "    # Sliding window to count n-grams\n",
    "    for line in corpus:\n",
    "        words = line.strip().split()\n",
    "        total_unigrams += len(words)\n",
    "        for i in range(len(words)):\n",
    "            unigram_counts[words[i]] += 1\n",
    "            for j in range(1, n+1):\n",
    "                if i + j <= len(words):\n",
    "                    ngram = tuple(words[i:i+j])\n",
    "                    ngram_counts[ngram] += 1\n",
    "                    if len(ngram) == n:\n",
    "                        total_ngrams += 1\n",
    "\n",
    "    # Step 2: Compute PMI for external n-grams\n",
    "    pmi_scores = {}\n",
    "    for ngram in external_ngrams:\n",
    "        ngram_tuple = tuple(ngram.split())\n",
    "        if len(ngram_tuple) != n:\n",
    "            raise ValueError(f\"External n-gram {ngram} does not match specified n ({n}).\")\n",
    "        \n",
    "        # Probability of n-gram\n",
    "        p_ngram = ngram_counts[ngram_tuple] / total_ngrams if total_ngrams > 0 else 0\n",
    "        \n",
    "        # Probability of individual unigrams\n",
    "        p_individuals = math.prod(\n",
    "            [unigram_counts[word] / total_unigrams for word in ngram_tuple]\n",
    "        )\n",
    "        \n",
    "        # Compute PMI\n",
    "        if p_ngram > 0 and p_individuals > 0:\n",
    "            pmi_scores[ngram] = math.log(p_ngram / p_individuals, 2)\n",
    "        else:\n",
    "            pmi_scores[ngram] = float('-inf')  # Log(0) case\n",
    "\n",
    "    return pmi_scores\n",
    "\n",
    "# Example usage\n",
    "corpus = [\n",
    "    \"this is a test corpus\",\n",
    "    \"this corpus is only a test\",\n",
    "    \"n-grams are useful for text processing\"\n",
    "]\n",
    "external_ngrams = [\"this is\", \"corpus is only\", \"text processing\"]\n",
    "\n",
    "# Compute PMI for all external n-grams, adjusting `n` dynamically\n",
    "for ngram in external_ngrams:\n",
    "    n = len(ngram.split())  # Dynamically adjust `n`\n",
    "    pmi_scores = compute_pmi(corpus, [ngram], n)\n",
    "    for ngram, pmi in pmi_scores.items():\n",
    "        print(f\"PMI({ngram}) = {pmi}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
