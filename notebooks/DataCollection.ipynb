{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Reference Corpus for baseline language statistics: [Middle English Public Domain Books](https://huggingface.co/datasets/PleIAs/English-PD):**\n",
    "<br>\n",
    "<br>\n",
    "Middle-English-Public Domain or Middle-English-PD is a large collection aiming to aggregate all midlle-age English monographies, periodicals and texts in the public domain. As of March 2024, it is the biggest middle-age English open corpus.\n",
    "\n",
    "*Now here, we should be using Project Gutenberg or something ideally, but we couldn't find a good parser for epubs. So we're again reverting to some assumptions in order to carry out some exploratory experiments.*\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# Load the dataset\n",
    "middle_english_pd = load_dataset(\n",
    "    \"PleIAs/Middle-English-PD\", \n",
    "    data_files=[f\"middle_english_pd_{i}.parquet\" for i in range(1, 2)], \n",
    "    split=\"train\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some general information about the dataset\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"The dataset has {len(middle_english_pd)} examples, and the publication dates of the dataset range between {min(middle_english_pd['publication_date'])} and {max(middle_english_pd['publication_date'])}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import json\n",
    "\n",
    "# write a function to print an example from the dataset in a readable format\n",
    "def print_wrapped_item(item, limit=80, trim_length=200):\n",
    "    def wrap_text(text, width):\n",
    "        return \"\\n\".join(textwrap.fill(line, width) for line in text.splitlines())\n",
    "\n",
    "    def trim_value(value, length):\n",
    "        if isinstance(value, str) and len(value) > length:\n",
    "            return value[:length] + \"...\"\n",
    "        return value\n",
    "    \n",
    "    # Prepare the item for pretty-printing with trimmed values\n",
    "    trimmed_item = {k: trim_value(v, trim_length) for k, v in item.items()}\n",
    "    pretty_json = json.dumps(trimmed_item, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    # Split the JSON string into lines and wrap each line\n",
    "    wrapped_lines = []\n",
    "    for line in pretty_json.splitlines():\n",
    "        wrapped_lines.extend(wrap_text(line, limit).splitlines())\n",
    "\n",
    "    # Print the wrapped lines\n",
    "    for line in wrapped_lines:\n",
    "        print(line)\n",
    "\n",
    "print(\"Example from the dataset:\")\n",
    "print_wrapped_item(middle_english_pd[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Transform the dataset into a list of dictionaries\n",
    "middle_english_pd = [\n",
    "    {\n",
    "        \"id\": entry[\"identifier\"],\n",
    "        \"text\": entry[\"title\"] + \"\\n\\n\" + entry[\"text\"],\n",
    "        \"source\": \"PleIAs/Middle-English-PD\"\n",
    "    }\n",
    "    for entry in tqdm(middle_english_pd, desc=\"Transforming dataset\")\n",
    "]\n",
    "\n",
    "print(f\"And the total number of words in the dataset is {round(sum(len(entry['text'].split()) for entry in middle_english_pd) / 1e6, 2)} million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_document(entry, n_low, n_high):\n",
    "    document_id = entry[\"id\"]\n",
    "    text = entry[\"text\"]\n",
    "\n",
    "    # TODO: Improve upon the text processing pipeline\n",
    "    # Split text into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    # remove special characters \n",
    "    sentences = [re.sub(r\"[^a-zA-Z0-9 ]\", \"\", sentence) for sentence in sentences]\n",
    "    # remove empty sentences\n",
    "    sentences = [sentence for sentence in sentences if sentence]\n",
    "    # make all sentences lowercase\n",
    "    sentences = [sentence.lower() for sentence in sentences]\n",
    "\n",
    "    # Initialize a dictionary for the current document's n-grams\n",
    "    document_ngram_dict = defaultdict(set)\n",
    "\n",
    "    # Process each sentence\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        words = sentence.split()\n",
    "\n",
    "        # Generate n-grams for the given range (n_low to n_high)\n",
    "        for n in range(n_low, n_high + 1):\n",
    "            ngrams = list(itertools.zip_longest(*[words[i:] for i in range(n)]))\n",
    "            ngrams = [\" \".join(ngram).strip() for ngram in ngrams if None not in ngram]\n",
    "\n",
    "            for ngram in ngrams:\n",
    "                document_ngram_dict[ngram].add(document_id)\n",
    "\n",
    "    return document_ngram_dict\n",
    "\n",
    "\"\"\"Converts the dataset into n-grams and saves the results to a file.\n",
    "\"\"\"\n",
    "def n_grams(dataset, n_low=2, n_high=5, file_path=\"../data/corpus/sentence_ngrams.json\"):\n",
    "    # Initialize a dictionary to store combined n-grams for all documents\n",
    "    combined_ngram_dict = defaultdict(lambda: {\"count\": 0, \"documents\": set()})\n",
    "\n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_document, entry, n_low, n_high) for entry in dataset]\n",
    "\n",
    "        # Use tqdm to show progress\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing documents\"):\n",
    "            document_ngram_dict = future.result()\n",
    "\n",
    "            # Combine document-level n-grams into the global combined dictionary\n",
    "            for ngram, document_ids in document_ngram_dict.items():\n",
    "                combined_ngram_dict[ngram][\"count\"] += len(document_ids)\n",
    "                combined_ngram_dict[ngram][\"documents\"].update(document_ids)\n",
    "\n",
    "    # Finalize the combined_ngram_dict to make it JSON serializable\n",
    "    processed_data = {\n",
    "        ngram: {\n",
    "            \"count\": data[\"count\"],\n",
    "            \"documents\": list(data[\"documents\"])\n",
    "        }\n",
    "        for ngram, data in combined_ngram_dict.items()\n",
    "    }\n",
    "\n",
    "    # Save the processed data to a file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(processed_data, f, indent=4)\n",
    "\n",
    "# Usage example:\n",
    "n_grams(middle_english_pd, n_low=2, n_high=5, file_path=\"../data/corpus/middle_english_pd_sentence_ngrams.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# clear memory\n",
    "gc.collect()\n",
    "# clear large variables\n",
    "del middle_english_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "**Social Media Corpus: [Reddit](https://huggingface.co/datasets/webis/tldr-17):**\n",
    "<br>\n",
    "<br>\n",
    "*Was originally supposed to be the [Conversational Reddits Datasets](https://github.com/PolyAI-LDN/conversational-datasets), but instead we switched to this because it's easier to download and parse through. The concept remains the same for exploratory experimentation.*\n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load 10% of the dataset\n",
    "reddit = load_dataset(\"webis/tldr-17\", trust_remote_code=True, split=\"train[:10%]\")\n",
    "# print some general information about the dataset\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"The dataset has {round(len(reddit)/1e6, 2)} million examples, and the publication dates range between 2006 and 2016.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example from the dataset:\")\n",
    "print_wrapped_item(reddit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset into a list of dictionaries\n",
    "reddit = [\n",
    "    {\n",
    "        \"id\": entry[\"id\"],\n",
    "        \"text\": entry[\"content\"],\n",
    "        \"source\": \"webis/tldr-17\"\n",
    "    }\n",
    "    for entry in tqdm(reddit, desc=\"Transforming dataset\")\n",
    "]\n",
    "\n",
    "print(f\"And the total number of words in the dataset is {round(sum(len(entry['text'].split()) for entry in reddit) / 1e6, 2)} million.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dataset into n-grams and save the results to a file\n",
    "n_grams(reddit, n_low=2, n_high=5, file_path=\"../data/corpus/reddit_sentence_ngrams.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# clear memory\n",
    "gc.collect()\n",
    "# clear large variables\n",
    "del reddit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
