{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 sentences matching 'machine learning':\n",
      "\t•There are hard (i.e., more interesting) problems to solve, and there can be a lot more math involved, especially if you are of the machine learning / artificial intelligence type.\n",
      "\t•Ironically however, if you could solve a problem like this, your best bet would be to have a machine learning algorithm try to figure it out.\n",
      "\t•Honestly I would like to see machine learning where your IFCS learns on the fly how to control the ship.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "\n",
    "# set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load 2% of the dataset: Can be increased for more diversity later\n",
    "reddit = load_dataset(\"webis/tldr-17\", trust_remote_code=True, split=\"train[:2%]\")\n",
    "corpus = [data[\"normalizedBody\"] for data in reddit]\n",
    "# shuffle the data\n",
    "random.shuffle(corpus)\n",
    "\n",
    "# Split into sentences and create a searchable structure\n",
    "sentences = []\n",
    "for text in corpus:\n",
    "    # Split on ., !, ? followed by space and capitalize letter\n",
    "    split_sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    sentences.extend([s.strip() for s in split_sentences if len(s.strip()) > 0])\n",
    "\n",
    "# Create chunks of 1000 sentences\n",
    "chunk_size = 1000\n",
    "sentence_chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "def find_matching_sentences(query_string, num_matches=3):\n",
    "    matches = []\n",
    "    for chunk in sentence_chunks:\n",
    "        for sentence in chunk:\n",
    "            if query_string.lower() in sentence.lower():\n",
    "                matches.append(sentence)\n",
    "                if len(matches) >= num_matches:\n",
    "                    return matches[:num_matches]\n",
    "    return matches\n",
    "\n",
    "# Test the function\n",
    "query = \"machine learning\"\n",
    "matches = find_matching_sentences(query)\n",
    "print(f\"Found {len(matches)} sentences matching '{query}':\")\n",
    "for match in matches:\n",
    "    print(f\"\\t•{match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:03<00:00, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of the dataset:\n",
      "3-gram:\n",
      "\t•a lot of: {'count': 49921, 'PMI': 13.69, 'sentences': ['They all have in common that Helsinki sucks a lot of work from the rest of Finland, meaning that the countryside is emptying and moving to Helsinki.', 'Personally having read them I feel a lot of it is a case of \"that doesn\\'t mean what you think it means\".', 'A very good documentary called \"because the bible tells me so\" has a lot of wonderful references to explain these verses better.']}\n",
      "\t•one of the: {'count': 27098, 'PMI': 1.53, 'sentences': ['By then I had been promoted to Shireikan and was one of the handful remaining senior officers of the Akodo.', 'The shitty parts came from the fact that we attended one of the wealthiest school districts in the country, so our peers and their parents were huge snobs and didn\\'t want to be around \"that kind of people\". (What kind?', 'If you want general file storage then buy that service (or use one of the many free services designed for that purpose) \\n Most of what I have said above applies to bandwidth also.']}\n",
      "\n",
      "4-gram:\n",
      "\t•the rest of the: {'count': 7528, 'PMI': 8.9, 'sentences': [\"So I grabbed my backpack, walked the rest of the way home, and washed up. \\n fast forward to the next Tuesday \\n ZadTheInhaler, you're wanted in the Principal's office \\n Fuck.\", 'Hidden throughout the rest of the room are the remaining gears, shrunken into item balls.', 'I heard her sigh, and then she pulled out her phone, and walked casually the rest of the way to class.']}\n",
      "\t•the end of the: {'count': 6781, 'PMI': 1.88, 'sentences': [\"It's the end of the school day - bell has rung, and everyone is heading to their lockers to grab their gear and head home, yours truly amongst the press.\", \"Those folks hiding out in the gas at the end of the match / using first aid kits to split the purse are in for a little shock :) \\n TL;DR - We're tweaking numbers to make the end-of-match smoother / quicker, and make the gas movement consistent throughout the match.\", 'At the end of the meal my dad talks to the manager and he notes their complaint but offers nothing in return for the bad service.']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load the n-gram datasets with the PMI scores\n",
    "target = {}\n",
    "\n",
    "for index in tqdm(range(3, 6)):\n",
    "    with open(f\"../data/corpus/reddit_10/{index}-gram_top_1000_pmi.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        target[index] = json.load(f)\n",
    "\n",
    "    # Find the sentences containing the n-grams from the corpus and add it to the dataset\n",
    "    for ngram in target[index]:\n",
    "        matches = find_matching_sentences(ngram)\n",
    "        target[index][ngram][\"sentences\"] = matches\n",
    "\n",
    "# Save the dataset\n",
    "with open(\"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target, f, indent=4)\n",
    "\n",
    "print(\"Structure of the dataset:\")\n",
    "# show first two entries\n",
    "for ngram in list(target.keys())[:2]:\n",
    "    print(f\"{ngram}-gram:\")\n",
    "    for key, value in list(target[ngram].items())[:2]:\n",
    "        print(f\"\\t•{key}: {value}\")\n",
    "        if key == \"sentences\":\n",
    "            print(f\"\\t\\t•{value[:2]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response by gemma2:latest to 'What is machine learning?' is: Imagine teaching a computer to learn without explicitly programming every single rule. That's essentially what machine learning is! \n",
      "\n",
      "It's a type of artificial intelligence where algorithms analyze data, identify patterns, and use those patterns to make predictions or decisions. Instead of being told exactly what to do, the algorithm learns from the data itself.\n",
      "\n",
      "Here's a simple analogy: Think of a child learning to recognize cats. You wouldn't write down every single feature of a cat (fur, whiskers, pointy ears, etc.). Instead, you'd show them pictures of cats and tell them \"This is a cat.\" Over time, the child learns to identify cats on their own based on the patterns they see.\n",
      "\n",
      "Machine learning works similarly. We feed algorithms massive amounts of data, and they learn to recognize patterns and make predictions.\n",
      "\n",
      "**Here are some key things to remember about machine learning:**\n",
      "\n",
      "* **Data-driven:** Machine learning relies heavily on data. The more data an algorithm has, the better it can learn.\n",
      "* **Algorithms:** These are the recipes that tell the computer how to learn from the data. There are many different types of algorithms, each suited for different tasks.\n",
      "* **Training:** This is the process of feeding data into an algorithm and allowing it to learn.\n",
      "\n",
      "**Examples of machine learning in action:**\n",
      "\n",
      "* **Recommender systems:** Netflix suggesting movies you might like based on your viewing history.\n",
      "* **Spam filtering:** Identifying unwanted emails.\n",
      "* **Image recognition:** Allowing your phone to recognize faces or objects in photos.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions!\n",
      "Imagine teaching a computer to learn without explicitly programming every single rule. That's essentially what machine learning is! \n",
      "\n",
      "It's a type of artificial intelligence where algorithms analyze data, identify patterns, and use those patterns to make predictions or decisions. Instead of being told exactly what to do, the algorithm learns from the data itself.\n",
      "\n",
      "Here's a simple analogy: Think of a child learning to recognize cats. You wouldn't write down every single feature of a cat (fur, whiskers, pointy ears, etc.). Instead, you'd show them pictures of cats and tell them \"This is a cat.\" Over time, the child learns to identify cats on their own based on the patterns they see.\n",
      "\n",
      "Machine learning works similarly. We feed algorithms massive amounts of data, and they learn to recognize patterns and make predictions.\n",
      "\n",
      "**Here are some key things to remember about machine learning:**\n",
      "\n",
      "* **Data-driven:** Machine learning relies heavily on data. The more data an algorithm has, the better it can learn.\n",
      "* **Algorithms:** These are the recipes that tell the computer how to learn from the data. There are many different types of algorithms, each suited for different tasks.\n",
      "* **Training:** This is the process of feeding data into an algorithm and allowing it to learn.\n",
      "\n",
      "**Examples of machine learning in action:**\n",
      "\n",
      "* **Recommender systems:** Netflix suggesting movies you might like based on your viewing history.\n",
      "* **Spam filtering:** Identifying unwanted emails.\n",
      "* **Image recognition:** Allowing your phone to recognize faces or objects in photos.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions!\n",
      "The response by qwen2.5:72b to 'Rate the quality of the response: Imagine teaching a computer to learn without explicitly programming every single rule. That's essentially what machine learning is! \n",
      "\n",
      "It's a type of artificial intelligence where algorithms analyze data, identify patterns, and use those patterns to make predictions or decisions. Instead of being told exactly what to do, the algorithm learns from the data itself.\n",
      "\n",
      "Here's a simple analogy: Think of a child learning to recognize cats. You wouldn't write down every single feature of a cat (fur, whiskers, pointy ears, etc.). Instead, you'd show them pictures of cats and tell them \"This is a cat.\" Over time, the child learns to identify cats on their own based on the patterns they see.\n",
      "\n",
      "Machine learning works similarly. We feed algorithms massive amounts of data, and they learn to recognize patterns and make predictions.\n",
      "\n",
      "**Here are some key things to remember about machine learning:**\n",
      "\n",
      "* **Data-driven:** Machine learning relies heavily on data. The more data an algorithm has, the better it can learn.\n",
      "* **Algorithms:** These are the recipes that tell the computer how to learn from the data. There are many different types of algorithms, each suited for different tasks.\n",
      "* **Training:** This is the process of feeding data into an algorithm and allowing it to learn.\n",
      "\n",
      "**Examples of machine learning in action:**\n",
      "\n",
      "* **Recommender systems:** Netflix suggesting movies you might like based on your viewing history.\n",
      "* **Spam filtering:** Identifying unwanted emails.\n",
      "* **Image recognition:** Allowing your phone to recognize faces or objects in photos.\n",
      "\n",
      "\n",
      "Let me know if you have any other questions!' is: The quality of the response is excellent. Here are some key points that make it so effective:\n",
      "\n",
      "1. **Clarity and Simplicity**: The explanation uses simple, understandable language to break down complex concepts. This makes the content accessible to beginners or those with no prior knowledge of machine learning.\n",
      "\n",
      "2. **Analogies**: The child-learning analogy is particularly strong. It effectively parallels how machine learning algorithms work, making the concept relatable and easier to grasp.\n",
      "\n",
      "3. **Key Points**: The response clearly outlines the main aspects of machine learning—data-driven nature, algorithms, and training—which are essential for understanding the core principles.\n",
      "\n",
      "4. **Examples**: Providing practical examples (recommender systems, spam filtering, image recognition) helps illustrate how machine learning is applied in real-world scenarios. This not only adds context but also shows the relevance and utility of the technology.\n",
      "\n",
      "5. **Structure**: The response has a logical flow, starting with a basic definition, moving to an analogy, then detailing key points, and finally providing examples. This structure makes it easy to follow and retain information.\n",
      "\n",
      "6. **Engagement**: The invitation at the end for further questions shows a willingness to engage, which is helpful for learners who might have additional queries or need more detailed explanations.\n",
      "\n",
      "Overall, this response is well-crafted and effective in explaining machine learning to someone new to the topic.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "import requests\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://kt-gpu5.ijs.si:11435/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "\n",
    "def create_chat_completions(model_name, message, timeout=100):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            timeout=timeout\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Timeout:\n",
    "        return None\n",
    "    except RequestException as e:\n",
    "        return None\n",
    "\n",
    "annotator_model = \"gemma2:latest\"\n",
    "judge_model = \"qwen2.5:72b\"\n",
    "\n",
    "# test the function\n",
    "message = \"What is machine learning?\"\n",
    "response = create_chat_completions(annotator_model, message)\n",
    "print(f\"The response by {annotator_model} to '{message}' is: {response}\")\n",
    "print(response)\n",
    "message = f\"Rate the quality of the response: {response}\"\n",
    "response = create_chat_completions(judge_model, message)\n",
    "print(f\"The response by {judge_model} to '{message}' is: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:28:05<00:00,  5.29s/it]\n",
      "100%|██████████| 1000/1000 [1:25:46<00:00,  5.15s/it]\n",
      "100%|██████████| 1000/1000 [1:24:23<00:00,  5.06s/it]\n",
      "100%|██████████| 3/3 [4:18:16<00:00, 5165.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def annotate_and_score_mwes(dataset, results_file, annotator_model, judge_model):\n",
    "    \"\"\"\n",
    "    Annotates and scores MWEs from the dataset using annotator and judge prompts.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (dict): The dataset containing n-grams and their details.\n",
    "        results_file (str): Path to the file where results should be written.\n",
    "        annotator_message (str): Template for the annotator prompt.\n",
    "        judge_message (str): Template for the judge prompt.\n",
    "        annotator_model (str): Name of the model to use for annotating chat completions.\n",
    "        judge_model (str): Name of the model to use for judging chat completions.\n",
    "    \"\"\"\n",
    "    for ngram, data in tqdm(dataset.items()):\n",
    "        for phrase, details in tqdm(data.items()):\n",
    "            # Extract data for the prompt\n",
    "            n_gram = phrase\n",
    "            frequency = details.get(\"count\", \"N/A\")\n",
    "            pmi_score = details.get(\"PMI\", \"N/A\")\n",
    "            sentences = details.get(\"sentences\", [])\n",
    "\n",
    "            # Limit to three sentences for prompt context\n",
    "            sentence_1 = sentences[0] if len(sentences) > 0 else \"No example sentence available.\"\n",
    "            sentence_2 = sentences[1] if len(sentences) > 1 else \"No example sentence available.\"\n",
    "            sentence_3 = sentences[2] if len(sentences) > 2 else \"No example sentence available.\"\n",
    "\n",
    "            annotator_message = f\"\"\"\n",
    "            You are an expert linguist helping to identify multi-word expressions (MWEs) in a large corpus. A multi-word expression is a sequence of words that form a single unit of meaning and cannot be easily deduced by the meanings of individual words.\n",
    "\n",
    "            **Significance of Pointwise Mutual Information (PMI):**\n",
    "            PMI is a statistical measure that quantifies how strongly two words are associated beyond what would be expected by random chance. A higher PMI score indicates that the words in the expression co-occur more frequently than expected, suggesting a meaningful relationship. PMI is particularly useful for identifying candidate MWEs as it captures statistical collocations that might signify idiomatic or fixed expressions.\n",
    "\n",
    "            Here is the information about a potential MWE:\n",
    "\n",
    "            - **Candidate Expression (n-gram):** \"{n_gram}\"\n",
    "            - **Frequency in Corpus:** {frequency}\n",
    "            - **Pointwise Mutual Information (PMI):** {pmi_score}\n",
    "\n",
    "            ### Example Sentences:\n",
    "            1. \"{sentence_1}\"\n",
    "            2. \"{sentence_2}\"\n",
    "            3. \"{sentence_3}\"\n",
    "\n",
    "            **Questions:**\n",
    "            1. Does this expression seem like a coherent unit of meaning in the context of the provided sentences? Explain why or why not.\n",
    "            2. Does replacing this n-gram with a single word or phrase preserve the overall meaning of the sentence? If so, suggest a possible replacement.\n",
    "            3. Does this expression appear to be idiomatic, collocational, or otherwise non-compositional? Provide specific reasons for your assessment.\n",
    "            4. Based on the above, would you classify this n-gram as a multi-word expression? (Yes/No). Justify your answer.\n",
    "            5. On a scale of 1-5, how confident are you in this classification? (1 = Not confident, 5 = Very confident). Provide reasoning for your confidence score.\n",
    "\n",
    "            Provide a detailed explanation for your answers, referencing the statistical significance (PMI score), frequency, and contextual usage in the examples.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            # Fill the annotator prompt\n",
    "            annotator_prompt = annotator_message.format(\n",
    "                n_gram=n_gram,\n",
    "                PMI_score=pmi_score,\n",
    "                frequency=frequency,\n",
    "                sentence_1=sentence_1,\n",
    "                sentence_2=sentence_2,\n",
    "                sentence_3=sentence_3\n",
    "            )\n",
    "\n",
    "            # Get the annotator's response\n",
    "            annotation = create_chat_completions(annotator_model, annotator_prompt)\n",
    "            if annotation is None:\n",
    "                annotation = \"Error: Annotation not available.\"\n",
    "            \n",
    "            \n",
    "            judge_message = f\"\"\"You are a judge evaluating the response of a linguist who has classified a multi-word expression (MWE) in a large corpus. Based on that, can you provide a label for the candidate phrase from the following options: \"Novel MWE\", \"Variation of an existing MWE\", \"not an MWE\"? Please provide only the label without any additional information.\"\"\"\n",
    "\n",
    "            # Fill the judge prompt\n",
    "            judge_prompt = judge_message.format(n_gram=n_gram)\n",
    "            \n",
    "            # Get the judge's response\n",
    "            judgement = create_chat_completions(judge_model, judge_prompt)\n",
    "            if judgement is None:\n",
    "                judgement = \"Error: Judgment not available.\"\n",
    "\n",
    "            # Prepare the result entry\n",
    "            result_entry = {\n",
    "                \"n_gram\": n_gram,\n",
    "                \"frequency\": frequency,\n",
    "                \"PMI_score\": pmi_score,\n",
    "                \"sentences\": [sentence_1, sentence_2, sentence_3],\n",
    "                \"annotation\": annotation,\n",
    "                \"judgement\": judgement\n",
    "            }\n",
    "\n",
    "            # Write the result to the file\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(json.dumps(result_entry) + \"\\n\")\n",
    "\n",
    "# Annotate and score the MWEs\n",
    "results_file = \"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences_results.json\"\n",
    "\n",
    "# test with a small subset of 10  n-grams per n = 2, 3, 4, 5\n",
    "# target = {k: dict(list(v.items())[:10]) for k, v in target.items()}\n",
    "\n",
    "# annotate_and_score_mwes\n",
    "annotate_and_score_mwes(target, results_file, annotator_model, judge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'n_gram': 'the shit out of', 'frequency': 5, 'PMI_score': 0.8, 'sentences': ['sentence_1', 'sentence_2'], 'annotation': ['example'], 'judgement': ['positive']}, {'n_gram': 'shit out of', 'frequency': 3, 'PMI_score': 0.6, 'sentences': ['sentence_3'], 'annotation': ['example'], 'judgement': ['neutral']}, {'n_gram': 'completely unexpected', 'frequency': 2, 'PMI_score': 0.9, 'sentences': ['sentence_4'], 'annotation': ['example'], 'judgement': ['positive']}]\n"
     ]
    }
   ],
   "source": [
    "def merge_ngrams(results):\n",
    "    from collections import defaultdict\n",
    "\n",
    "    def is_subsequence(sub, main):\n",
    "        \"\"\"Check if `sub` is a subsequence of `main`.\"\"\"\n",
    "        sub_len, main_len = len(sub), len(main)\n",
    "        for i in range(main_len - sub_len + 1):\n",
    "            if main[i:i + sub_len] == sub:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Group results by unique n-grams.\n",
    "    merged_results = []\n",
    "    merged_map = defaultdict(list)\n",
    "\n",
    "    for result in results:\n",
    "        n_gram = result[\"n_gram\"].split()\n",
    "        found = False\n",
    "\n",
    "        # Check if the n-gram can be merged with an existing group.\n",
    "        for existing_n_gram, group in list(merged_map.items()):\n",
    "            if is_subsequence(n_gram, existing_n_gram) or is_subsequence(existing_n_gram, n_gram):\n",
    "                # Merge the two groups.\n",
    "                merged_map[existing_n_gram].append(result)\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            # Add a new group for this n-gram.\n",
    "            merged_map[tuple(n_gram)].append(result)\n",
    "\n",
    "    # Consolidate the merged results.\n",
    "    for n_grams, group in merged_map.items():\n",
    "        merged_result = {\n",
    "            \"n_gram\": \" \".join(n_grams),\n",
    "            \"frequency\": sum(item[\"frequency\"] for item in group),\n",
    "            \"PMI_score\": sum(item[\"PMI_score\"] for item in group) / len(group),\n",
    "            \"sentences\": list(set(sentence for item in group for sentence in item[\"sentences\"])),\n",
    "            \"annotation\": list(set(item[\"annotation\"] for item in group)),\n",
    "            \"judgement\": list(set(item[\"judgement\"] for item in group))\n",
    "        }\n",
    "        merged_results.append(merged_result)\n",
    "\n",
    "    return merged_results\n",
    "\n",
    "# Example Usage:\n",
    "results = [\n",
    "    {\n",
    "        \"n_gram\": \"the shit out of\",\n",
    "        \"frequency\": 5,\n",
    "        \"PMI_score\": 0.8,\n",
    "        \"sentences\": [\"sentence_1\", \"sentence_2\"],\n",
    "        \"annotation\": \"example\",\n",
    "        \"judgement\": \"positive\"\n",
    "    },\n",
    "    {\n",
    "        \"n_gram\": \"shit out of\",\n",
    "        \"frequency\": 3,\n",
    "        \"PMI_score\": 0.6,\n",
    "        \"sentences\": [\"sentence_3\"],\n",
    "        \"annotation\": \"example\",\n",
    "        \"judgement\": \"neutral\"\n",
    "    },\n",
    "    {\n",
    "        \"n_gram\": \"completely unexpected\",\n",
    "        \"frequency\": 2,\n",
    "        \"PMI_score\": 0.9,\n",
    "        \"sentences\": [\"sentence_4\"],\n",
    "        \"annotation\": \"example\",\n",
    "        \"judgement\": \"positive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "merged_results = merge_ngrams(results)\n",
    "print(merged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique judgements: {'\"not an MWE\"', '[\"Novel MWE\"]', 'not an MWE', 'Novel MWE', 'Variation of an existing MWE', '\"Novel MWE\"', '[\"not an MWE\"]', '\"Variation of an existing MWE\"', 'novel MWE', 'Not an MWE', '[\"Not an MWE\"]'}\n",
      "Number of results: 5605\n",
      "Number of novel MWEs found: 53\n",
      "Example novel MWEs:\n",
      "\t•im not saying\n",
      "\t\t•Frequency: 2718\n",
      "\t\t•PMI Score: 18.42\n",
      "\t\t•Sentences:\n",
      "\t\t\t•Im not saying they dont need advocacy.\n",
      "\t\t\t•Now Im not saying that it's a bad thing that people are putting their health and future well being (possibly) on the line for the possibility of making millions of dollars.\n",
      "\t\t\t•But a cliffhanger people seem to leave out is that most of his rank1 was archieved by duo-Q recently a good example would be his r1 at the moment (Apdo dog2) Duo-Q a shitton with Rekkles and WhiteKnight108 (Another pretty sick soloq guy) \n",
      " Im not saying he wouldnt archieve rank1 without, but it certainly helped.\n",
      "\t•shit out of\n",
      "\t\t•Frequency: 2055\n",
      "\t\t•PMI Score: 18.01\n",
      "\t\t•Sentences:\n",
      "\t\t\t•Beat the shit out of me, on my own.\n",
      "\t\t\t•I'm upvoting the shit out of anyone who recognizes this beer.\n",
      "\t\t\t•The villains scared the shit out of you.\n",
      "\t•at this point i\n",
      "\t\t•Frequency: 1936\n",
      "\t\t•PMI Score: 17.93\n",
      "\t\t•Sentences:\n",
      "\t\t\t•At this point I still had maybe a good hour or so until I made it home, and its pouring down rain so keep driving and I can't see out my windshield at all.\n",
      "\t\t\t•At this point I think it's just a massive toe/foot cramp, so I take my shoe off to look.\n",
      "\t\t\t•At this point I decide to try and get out there socially to perk my self up a bit.\n",
      "\t•the problem is that\n",
      "\t\t•Frequency: 1440\n",
      "\t\t•PMI Score: 17.5\n",
      "\t\t•Sentences:\n",
      "\t\t\t•The problem is that there is no such thing as \"basics\" when it comes to complex situations like yours.\n",
      "\t\t\t•Tl;dr:  The problem isn't 'new technologies' -- I'm not a Luddite --, the problem is that the new technologies aren't actually better than old-tech, and may actually even be worse.\n",
      "\t\t\t•For me the problem is that assault is sometimes balanced...but only if the average standard of player on both teams is good.\n",
      "\t•felt like i was\n",
      "\t\t•Frequency: 729\n",
      "\t\t•PMI Score: 16.52\n",
      "\t\t•Sentences:\n",
      "\t\t\t•I'm not the type to slack off on the job, but I felt like I was just in crunch time mode from hour one through hour 8, just so I could get as much money as I could.\n",
      "\t\t\t•I was just laying on my side, on the ground, so I looked still and relatively manageable to everyone around me, but what I couldn't communicate was that my brain was producing such hideous hallucinations and scary visuals that I literally felt like I was being slammed back and forth on the ground like a rag doll.\n",
      "\t\t\t•not a doctor, but i just had to share my canadian health care system experience with near-death  ; \n",
      " i've been feeling weak and worn out for a few days, sleeping 12 hours a day and just looking like total shit. i took it upon myself to get it checked out, im usually not into doctor visits and avoid it, but i hit up the casual walk-in clinic 2 streets away. the doc called it \"food poisoning\" & suggested i go home and get some sleep. (but they did take a blood test as a precautionary measure) \n",
      " so i went home. had to take a break to sit down because i was out of breath after about 250 feet (i lived maybe 500 feet from the clinic so lucky me) so i get inside my house and go take a shit because i didnt want to sleep with that little annoying pressure in my abdomen, so i destroy the toilet, and literally, just shit a small car, completely emptied i felt like i was a new person. i stand up, im feeling dizzy, out of breath, i look in the bowl,  ITS FILLED WITH BLACK LIQUID \n",
      "my sight immediately shutters and i feel like im going to pass out, i stumble through the hallway toward my room, i lose conciousness for a second, fall to my knees, get up, stumble against the wall toward my bed, drop onto the mattress which was the most comfortable thing ive ever felt. (it would rival with an extasy high) so now im in my bed, and i just lose it, dont remember anything until i am being shaken awake by the doctor who diagnosed me with \"food poisoning\" a few minutes earlier.\n",
      "(it was actually a few hours later when my blood test results came in) \n",
      " i later found out that; \n",
      " when my blood tests came back, the doctor who diagnosed me that day looked at my results  AND FREAKED THE FUCK OUT , LEFT THE CLINIC, RAN TO MY HOUSE, BROKE THE WINDOW IN THE BACKYARD TO OPEN THE DOOR, RAN UPSTAIRS AND WOKE ME UP AND GOT ME READY FOR THE AMBULANCE RIDE.\n",
      "********************************************************************************\n",
      "\n",
      "Number of variations found: 277\n",
      "Example variations:\n",
      "\t•i ended up\n",
      "\t\t•Frequency: 4687\n",
      "\t\t•PMI Score: 19.2\n",
      "\t\t•Sentences:\n",
      "\t\t\t•I ended up applying to the local college to stay close to home and not make new friends.\n",
      "\t\t\t•I ended up being placed in a 10 person apartment where we all shared a kitchen living room and dining room.\n",
      "\t\t\t•I ended up winning employee of the year for my position and got a pay raise for the summer maintenance job they asked me to do (with no experience, solely based on my critical eye while bored at work/going above and beyond).\n",
      "\t•it was just\n",
      "\t\t•Frequency: 3162\n",
      "\t\t•PMI Score: 18.64\n",
      "\t\t•Sentences:\n",
      "\t\t\t•Easier just to write: Kill The Masters. \n",
      " tl;dr Stop reading so deeply into it, it was just a translation for the audience.\n",
      "\t\t\t•It was just too amazing to leave any details out.\n",
      "\t\t\t•His dad called and left a furious voicemail on his phone...it snowballed out of control and we wound up paying for the rehearsal dinner and never getting a wedding gift from them (not that I care about stuff, it was just shitty of them to give their son nothing).\n",
      "\t•as opposed to\n",
      "\t\t•Frequency: 2260\n",
      "\t\t•PMI Score: 18.15\n",
      "\t\t•Sentences:\n",
      "\t\t\t•It's enforcced by praising kids for their hard work (as opposed to their intelligence).\n",
      "\t\t\t•The part that was server-side ran on Microsoft's infra though (as opposed to Bungie or Activision or whatever hosting company/companies they are currently using).\n",
      "\t\t\t•In fact the term 'Britons' was used to describe the Celtic peoples of Wales, Scotland and the South West  long before the island was united, as opposed to the jumble of Saxon, Dane and Norman and other groups that populated most of England at the time.\n",
      "\t•i didnt want to\n",
      "\t\t•Frequency: 2195\n",
      "\t\t•PMI Score: 18.11\n",
      "\t\t•Sentences:\n",
      "\t\t\t•I probably shouldve worded my post better, but i did so because i didnt want to sound like a noob who was destroying my newly built computer, but here it goes.\n",
      "\t\t\t•But I dont agree with the article that it is easy or even possible to do in minutes, when I decided to increase my ethical shot distance I took alot of factors in mind, because I didnt want to he the one responsible for wounding and losing an animal or causing needless suffering.\n",
      "\t\t\t•not a doctor, but i just had to share my canadian health care system experience with near-death  ; \n",
      " i've been feeling weak and worn out for a few days, sleeping 12 hours a day and just looking like total shit. i took it upon myself to get it checked out, im usually not into doctor visits and avoid it, but i hit up the casual walk-in clinic 2 streets away. the doc called it \"food poisoning\" & suggested i go home and get some sleep. (but they did take a blood test as a precautionary measure) \n",
      " so i went home. had to take a break to sit down because i was out of breath after about 250 feet (i lived maybe 500 feet from the clinic so lucky me) so i get inside my house and go take a shit because i didnt want to sleep with that little annoying pressure in my abdomen, so i destroy the toilet, and literally, just shit a small car, completely emptied i felt like i was a new person. i stand up, im feeling dizzy, out of breath, i look in the bowl,  ITS FILLED WITH BLACK LIQUID \n",
      "my sight immediately shutters and i feel like im going to pass out, i stumble through the hallway toward my room, i lose conciousness for a second, fall to my knees, get up, stumble against the wall toward my bed, drop onto the mattress which was the most comfortable thing ive ever felt. (it would rival with an extasy high) so now im in my bed, and i just lose it, dont remember anything until i am being shaken awake by the doctor who diagnosed me with \"food poisoning\" a few minutes earlier.\n",
      "(it was actually a few hours later when my blood test results came in) \n",
      " i later found out that; \n",
      " when my blood tests came back, the doctor who diagnosed me that day looked at my results  AND FREAKED THE FUCK OUT , LEFT THE CLINIC, RAN TO MY HOUSE, BROKE THE WINDOW IN THE BACKYARD TO OPEN THE DOOR, RAN UPSTAIRS AND WOKE ME UP AND GOT ME READY FOR THE AMBULANCE RIDE.\n",
      "\t•and i didnt\n",
      "\t\t•Frequency: 2179\n",
      "\t\t•PMI Score: 18.1\n",
      "\t\t•Sentences:\n",
      "\t\t\t•He put his head back down and i didnt think much of it.\n",
      "\t\t\t•As soon as I did that I started gettin chased by everyone and i didnt know what to do afterwards to make them stop so I ended it there and never played again. \n",
      " tl;dr got scared and mad that i was getting chased so i stopped playing\n",
      "\t\t\t•i was a retarded awkward 15 year old and couldn't figure out if this girl that i really liked liked me, due to a severe fear of rejection i wanted to be sure. we went into the city with a few friends one day (5 of us) and at one point some guy comes up to us trying to sell us something or whatever, we started talking to the guy and the girl refered to me as \"her man\" and when the guy joked that i was cute she said she'd fight him for me... dont worry it gets worse. we then went to 7/11 and i was getting a slurpee but wanted to try some of the flavours so i put 2 on the ends of my fingers and try them, she then says \"ooh let me try\" so i put the same flavours on my fingers and think she's just going to kinda lick them off the tips of my fingers... she basically deepthroated my fingers 1 by 1, i got a mad boner but did nothing. \n",
      " about a month later my parents were away for a couple weeks so i had some friends over, she said she'd come but she was having dinner with her dad who lives in a different country so she'd be a bit late. she'd had a saki drinking contest with her dad (i know best dad ever) so she comes in and was sitting on my lap the whole time pretty much all over me, it gets to later that night and she's completely sobered up (its been about 6 hours) and we'd decided to go to sleep, i offered to let her stay in my bed but she doesnt like sleeping alone without any sound (she usually has the TV on at home... long story). so she stayed down stairs with us. no biggy except she went to the bathroom to get changed and came out in just a really sexy black bra and some really small silk shorts. she's about 5'6\" and probably weighs less than 50kg but has amazing DDs so this was a great sight. as i was sleeping on a crappy beanbag and another friend was on a large inflatable matress she slept with him, i stayed awake and at about 7am i get on my beanbag and after about 2 minutes she gets up out of the bed with the other guy and without saying anything pulls my sheet off, lays down next to me and puts my arms in a spooning position, this is not some huge beanbag so i'm like breathing on her neck and trying to control my heart rate and breathing... and avoid my boner sticking into her back. 2 months later she'd given up on me and started going out with the guy she originally shared the bed with, he asked me if i was ok with it first, i almost stabbed him but said i was cool with it. \n",
      " tl;dr chick called me her boyfriend to strangers, deepthroated my fingers, slept half naked making me spoon her and in doing so rejecting sleeping in a much bigger bed with another guy and i didnt get the fucking hint \n",
      " edit: shortened a little, edit: shortened a little more\n",
      "********************************************************************************\n",
      "\n",
      "Number of non-MWEs found: 3847\n",
      "Example non-MWEs:\n",
      "\t•i just\n",
      "\t•i started\n",
      "\t•based on\n",
      "\t•i couldnt\n",
      "\t•it wasnt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the results\n",
    "results_file = \"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences_results.json\"\n",
    "\n",
    "# Load the results\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    results = [json.loads(line) for line in f]\n",
    "\n",
    "# Standardize the judgment key spelling\n",
    "for result in results:\n",
    "    # Check for different spellings and standardize to \"judgement\"\n",
    "    if \"judgment\" in result:\n",
    "        result[\"judgement\"] = result.pop(\"judgment\")\n",
    "    elif \"jugdement\" in result:\n",
    "        result[\"judgement\"] = result.pop(\"jugdement\")\n",
    "\n",
    "# unique judgements\n",
    "judgements = set(result[\"judgement\"] for result in results)\n",
    "print(f\"Unique judgements: {judgements}\")\n",
    "\n",
    "print(f\"Number of results: {len(results)}\")\n",
    "\n",
    "# {'\"not an MWE\"', '[\"Novel MWE\"]', 'not an MWE', 'Novel MWE', 'Variation of an existing MWE', '\"Novel MWE\"', '[\"not an MWE\"]', '\"Variation of an existing MWE\"', 'novel MWE', 'Not an MWE', '[\"Not an MWE\"]'} -> 3 unique judgements = [\"Novel MWE\", \"Variation of an existing MWE\", \"not an MWE\"]\n",
    "# Standardize the judgement labels\n",
    "judgement_mapping = {\n",
    "    \"novel MWE\": \"Novel MWE\",\n",
    "    \"Novel MWE\": \"Novel MWE\",\n",
    "    \"Variation of an existing MWE\": \"Variation of an existing MWE\",\n",
    "    \"not an MWE\": \"not an MWE\",\n",
    "    \"Not an MWE\": \"not an MWE\",\n",
    "    \"Not an MWE\": \"not an MWE\",\n",
    "    \"['Novel MWE']\": \"Novel MWE\",\n",
    "    \"['Variation of an existing MWE']\": \"Variation of an existing MWE\",\n",
    "    \"['not an MWE']\": \"not an MWE\"\n",
    "}\n",
    "\n",
    "for result in results:\n",
    "    if result[\"judgement\"] in judgement_mapping:\n",
    "        result[\"judgement\"] = judgement_mapping[result[\"judgement\"]]\n",
    "\n",
    "# sort the results by high PMI scores\n",
    "results = sorted(results, key=lambda x: x[\"PMI_score\"], reverse=True)\n",
    "\n",
    "# Now we can safely access \"judgement\"\n",
    "novel_mwes = [result for result in results if \"judgement\" in result and \"Novel MWE\" in result[\"judgement\"]]\n",
    "variations = [result for result in results if \"judgement\" in result and \"Variation of an existing MWE\" in result[\"judgement\"]]\n",
    "not_mwes = [result for result in results if \"judgement\" in result and \"not an MWE\" in result[\"judgement\"]]\n",
    "\n",
    "# shorten the lists so that if sequence abc is novel, then xabc or xyabc is considered only once\n",
    "novel_mwes = merge_ngrams(novel_mwes)\n",
    "variations = merge_ngrams(variations)\n",
    "not_mwes = merge_ngrams(not_mwes)\n",
    "\n",
    "print(f\"Number of novel MWEs found: {len(novel_mwes)}\")\n",
    "if len(novel_mwes) > 0:\n",
    "    print(f\"Example novel MWEs:\")\n",
    "    for result in novel_mwes[:5]:  # Show first 5 examples\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        print(f\"\\t\\t•Frequency: {result['frequency']}\")\n",
    "        print(f\"\\t\\t•PMI Score: {result['PMI_score']}\")\n",
    "        print(f\"\\t\\t•Sentences:\")\n",
    "        for sentence in result['sentences']:\n",
    "            print(f\"\\t\\t\\t•{sentence}\")\n",
    "        # print(f\"\\t\\t•Annotation: {result['annotation']}\")\n",
    "\n",
    "print(f\"{'*'*80}\")\n",
    "\n",
    "print(f\"\\nNumber of variations found: {len(variations)}\")\n",
    "if len(variations) > 0:\n",
    "    print(f\"Example variations:\")\n",
    "    for result in variations[:5]:  # Show first 5 examples\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        print(f\"\\t\\t•Frequency: {result['frequency']}\")\n",
    "        print(f\"\\t\\t•PMI Score: {result['PMI_score']}\")\n",
    "        print(f\"\\t\\t•Sentences:\")\n",
    "        for sentence in result['sentences']:\n",
    "            print(f\"\\t\\t\\t•{sentence}\")\n",
    "        # print(f\"\\t\\t•Annotation: {result['annotation']}\")\n",
    "\n",
    "print(f\"{'*'*80}\")\n",
    "\n",
    "print(f\"\\nNumber of non-MWEs found: {len(not_mwes)}\")\n",
    "if len(not_mwes) > 0:\n",
    "    print(f\"Example non-MWEs:\")\n",
    "    for result in not_mwes[:5]:  # Show first 5 examples\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        # print(f\"\\t\\t•Annotation: {result['annotation']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
