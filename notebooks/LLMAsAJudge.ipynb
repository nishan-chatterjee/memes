{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 sentences matching 'machine learning':\n",
      "\t•There are hard (i.e., more interesting) problems to solve, and there can be a lot more math involved, especially if you are of the machine learning / artificial intelligence type.\n",
      "\t•Ironically however, if you could solve a problem like this, your best bet would be to have a machine learning algorithm try to figure it out.\n",
      "\t•Honestly I would like to see machine learning where your IFCS learns on the fly how to control the ship.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import random\n",
    "\n",
    "# set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Load 2% of the dataset: Can be increased for more diversity later\n",
    "reddit = load_dataset(\"webis/tldr-17\", trust_remote_code=True, split=\"train[:2%]\")\n",
    "corpus = [data[\"normalizedBody\"] for data in reddit]\n",
    "# shuffle the data\n",
    "random.shuffle(corpus)\n",
    "\n",
    "# Split into sentences and create a searchable structure\n",
    "sentences = []\n",
    "for text in corpus:\n",
    "    # Split on ., !, ? followed by space and capitalize letter\n",
    "    split_sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
    "    sentences.extend([s.strip() for s in split_sentences if len(s.strip()) > 0])\n",
    "\n",
    "# Create chunks of 1000 sentences\n",
    "chunk_size = 1000\n",
    "sentence_chunks = [sentences[i:i + chunk_size] for i in range(0, len(sentences), chunk_size)]\n",
    "\n",
    "def find_matching_sentences(query_string, num_matches=3):\n",
    "    matches = []\n",
    "    for chunk in sentence_chunks:\n",
    "        for sentence in chunk:\n",
    "            if query_string.lower() in sentence.lower():\n",
    "                matches.append(sentence)\n",
    "                if len(matches) >= num_matches:\n",
    "                    return matches[:num_matches]\n",
    "    return matches\n",
    "\n",
    "# Test the function\n",
    "query = \"machine learning\"\n",
    "matches = find_matching_sentences(query)\n",
    "print(f\"Found {len(matches)} sentences matching '{query}':\")\n",
    "for match in matches:\n",
    "    print(f\"\\t•{match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:04<00:00, 16.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure of the dataset:\n",
      "2-gram:\n",
      "\t•of the: {'count': 295160, 'PMI': -0.83, 'sentences': [\"You know, people go to all the trouble of writing and updating the FAQ, just so that people like you, won't make fools of themselves by making bold, confidant, assertions, without thinking it through, or doing even a little bit of research.\", 'Further, to suggest that anti theism has anything to do with hating people demonstrates breath taking ignorance of the subject, or a malicious attempt to defame and inflame.', 'Now, even if you disagree with every single thing he said, surely you must realise that it would have been impossible for him to achieve the success he did as an author and public intellectual if he had tried to maintain a position of hating people because of their affiliations.']}\n",
      "\t•in the: {'count': 261647, 'PMI': 0.09, 'sentences': [\"The land on the other side of the Eastern Peninsula is being developed, but it's far from the centre (and has two bottlenecks in the sea), so it's easier to just drive a car to the centre from the neighbouring cities on the said highways.\", 'And land in there is near the coast as well and pretty dear, so city wants to get the best possible price out of it.', \"Doesn't help that there are only so many builders and if too many are employed at the same time, they start to use less-trained workforce (eg. subsubcontracting Baltics who aren't going to protest when the Finnish contractor starts cutting corners in the cement-drying time, using bad-quality parts etc.) \\n \\n \\n \\n TL:DR; There is only so much available land and you can get a good price on it, so why build housing for lower middle-class or for the working classes?\"]}\n",
      "\n",
      "3-gram:\n",
      "\t•a lot of: {'count': 49921, 'PMI': 13.69, 'sentences': ['They all have in common that Helsinki sucks a lot of work from the rest of Finland, meaning that the countryside is emptying and moving to Helsinki.', 'Personally having read them I feel a lot of it is a case of \"that doesn\\'t mean what you think it means\".', 'A very good documentary called \"because the bible tells me so\" has a lot of wonderful references to explain these verses better.']}\n",
      "\t•one of the: {'count': 27098, 'PMI': 1.53, 'sentences': ['By then I had been promoted to Shireikan and was one of the handful remaining senior officers of the Akodo.', 'The shitty parts came from the fact that we attended one of the wealthiest school districts in the country, so our peers and their parents were huge snobs and didn\\'t want to be around \"that kind of people\". (What kind?', 'If you want general file storage then buy that service (or use one of the many free services designed for that purpose) \\n Most of what I have said above applies to bandwidth also.']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# load the n-gram datasets with the PMI scores\n",
    "target = {}\n",
    "\n",
    "for index in tqdm(range(2, 6)):\n",
    "    with open(f\"../data/corpus/reddit_10/{index}-gram_top_1000_pmi.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        target[index] = json.load(f)\n",
    "\n",
    "    # Find the sentences containing the n-grams from the corpus and add it to the dataset\n",
    "    for ngram in target[index]:\n",
    "        matches = find_matching_sentences(ngram)\n",
    "        target[index][ngram][\"sentences\"] = matches\n",
    "\n",
    "# Save the dataset\n",
    "with open(\"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(target, f, indent=4)\n",
    "\n",
    "print(\"Structure of the dataset:\")\n",
    "# show first two entries\n",
    "for ngram in list(target.keys())[:2]:\n",
    "    print(f\"{ngram}-gram:\")\n",
    "    for key, value in list(target[ngram].items())[:2]:\n",
    "        print(f\"\\t•{key}: {value}\")\n",
    "        if key == \"sentences\":\n",
    "            print(f\"\\t\\t•{value[:2]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response by gemma2:latest to 'What is machine learning?' is: Imagine teaching a dog a new trick. You show it what to do, reward it when it gets close, and correct it when it's wrong. Over time, the dog learns the trick through practice and feedback.\n",
      "\n",
      "Machine learning is similar! It's a type of artificial intelligence where computers learn from data instead of being explicitly programmed. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Data:**  Just like the dog needs examples, machine learning algorithms need lots of data to learn from. This data can be anything: images, text, numbers, sound recordings, etc.\n",
      "* **Algorithm:** This is the set of rules the computer uses to process the data and find patterns. Think of it as the training plan for the algorithm.\n",
      "* **Model:**  After learning from the data, the algorithm creates a \"model\" which can make predictions or decisions on new, unseen data.\n",
      "\n",
      "**Here are some examples of machine learning in action:**\n",
      "\n",
      "* **Recommendation systems:** Netflix suggesting movies you might like based on your past viewing history.\n",
      "* **Spam filters:** Identifying and blocking unwanted emails.\n",
      "* **Self-driving cars:**  Recognizing objects and making driving decisions.\n",
      "* ** médicale diagnosis:** Helping doctors diagnose diseases by analyzing medical images.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "* **Supervised learning:** The algorithm is trained on labeled data (e.g., images labeled \"cat\" or \"dog\").\n",
      "* **Unsupervised learning:** The algorithm finds patterns in unlabeled data (e.g., grouping customers based on their purchasing behavior). \n",
      "* **Reinforcement learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones (e.g., teaching a robot to walk).\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any of these concepts in more detail!\n",
      "Imagine teaching a dog a new trick. You show it what to do, reward it when it gets close, and correct it when it's wrong. Over time, the dog learns the trick through practice and feedback.\n",
      "\n",
      "Machine learning is similar! It's a type of artificial intelligence where computers learn from data instead of being explicitly programmed. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Data:**  Just like the dog needs examples, machine learning algorithms need lots of data to learn from. This data can be anything: images, text, numbers, sound recordings, etc.\n",
      "* **Algorithm:** This is the set of rules the computer uses to process the data and find patterns. Think of it as the training plan for the algorithm.\n",
      "* **Model:**  After learning from the data, the algorithm creates a \"model\" which can make predictions or decisions on new, unseen data.\n",
      "\n",
      "**Here are some examples of machine learning in action:**\n",
      "\n",
      "* **Recommendation systems:** Netflix suggesting movies you might like based on your past viewing history.\n",
      "* **Spam filters:** Identifying and blocking unwanted emails.\n",
      "* **Self-driving cars:**  Recognizing objects and making driving decisions.\n",
      "* ** médicale diagnosis:** Helping doctors diagnose diseases by analyzing medical images.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "* **Supervised learning:** The algorithm is trained on labeled data (e.g., images labeled \"cat\" or \"dog\").\n",
      "* **Unsupervised learning:** The algorithm finds patterns in unlabeled data (e.g., grouping customers based on their purchasing behavior). \n",
      "* **Reinforcement learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones (e.g., teaching a robot to walk).\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any of these concepts in more detail!\n",
      "The response by qwen2.5:72b to 'Rate the quality of the response: Imagine teaching a dog a new trick. You show it what to do, reward it when it gets close, and correct it when it's wrong. Over time, the dog learns the trick through practice and feedback.\n",
      "\n",
      "Machine learning is similar! It's a type of artificial intelligence where computers learn from data instead of being explicitly programmed. \n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "* **Data:**  Just like the dog needs examples, machine learning algorithms need lots of data to learn from. This data can be anything: images, text, numbers, sound recordings, etc.\n",
      "* **Algorithm:** This is the set of rules the computer uses to process the data and find patterns. Think of it as the training plan for the algorithm.\n",
      "* **Model:**  After learning from the data, the algorithm creates a \"model\" which can make predictions or decisions on new, unseen data.\n",
      "\n",
      "**Here are some examples of machine learning in action:**\n",
      "\n",
      "* **Recommendation systems:** Netflix suggesting movies you might like based on your past viewing history.\n",
      "* **Spam filters:** Identifying and blocking unwanted emails.\n",
      "* **Self-driving cars:**  Recognizing objects and making driving decisions.\n",
      "* ** médicale diagnosis:** Helping doctors diagnose diseases by analyzing medical images.\n",
      "\n",
      "**Types of Machine Learning:**\n",
      "\n",
      "* **Supervised learning:** The algorithm is trained on labeled data (e.g., images labeled \"cat\" or \"dog\").\n",
      "* **Unsupervised learning:** The algorithm finds patterns in unlabeled data (e.g., grouping customers based on their purchasing behavior). \n",
      "* **Reinforcement learning:** The algorithm learns through trial and error, receiving rewards for correct actions and penalties for incorrect ones (e.g., teaching a robot to walk).\n",
      "\n",
      "\n",
      "Let me know if you'd like to explore any of these concepts in more detail!' is: The response is quite comprehensive and well-structured. It effectively uses an analogy to introduce the concept of machine learning, making it accessible and relatable. Here’s a detailed breakdown:\n",
      "\n",
      "### Strengths:\n",
      "1. **Analogy:** Using the dog training example makes the concept of machine learning more understandable for beginners.\n",
      "2. **Clarity:** The explanation is clear and concise, breaking down complex terms into simpler concepts.\n",
      "3. **Structure:**\n",
      "   - **Data, Algorithm, Model:** These key components are clearly defined.\n",
      "   - **Examples:** Real-world applications (Netflix recommendations, spam filters, self-driving cars, medical diagnosis) help illustrate the practical uses of machine learning.\n",
      "   - **Types of Machine Learning:** The different types are explained with clear examples, making it easier to grasp each concept.\n",
      "4. **Engagement:** The invitation to explore concepts in more detail encourages further interaction and learning.\n",
      "\n",
      "### Areas for Improvement:\n",
      "1. **Technical Depth:**\n",
      "   - While the response is great for beginners, it could benefit from a bit more technical depth for those who want to delve deeper. For example, explaining what \"labeled data\" means or providing a simple example of an algorithm.\n",
      "2. **Grammar and Typos:**\n",
      "   - There is a minor typo: \"médicale diagnosis\" should be \"medical diagnosis.\"\n",
      "3. **Flow:**\n",
      "   - The transition between the different types of machine learning could be smoother to maintain the reader's attention.\n",
      "\n",
      "### Overall Rating:\n",
      "I would rate the quality of this response as **very good** (8.5/10). It effectively communicates the basics of machine learning in an engaging and understandable manner, with room for a bit more depth and minor grammatical corrections.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "import requests\n",
    "from requests.exceptions import Timeout, RequestException\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url='http://kt-gpu5.ijs.si:11435/v1',\n",
    "    api_key='ollama',  # required, but unused\n",
    ")\n",
    "\n",
    "def create_chat_completions(model_name, message, timeout=100):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": message},\n",
    "            ],\n",
    "            timeout=timeout\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Timeout:\n",
    "        return None\n",
    "    except RequestException as e:\n",
    "        return None\n",
    "\n",
    "annotator_model = \"gemma2:latest\"\n",
    "judge_model = \"qwen2.5:72b\"\n",
    "\n",
    "# test the function\n",
    "message = \"What is machine learning?\"\n",
    "response = create_chat_completions(annotator_model, message)\n",
    "print(f\"The response by {annotator_model} to '{message}' is: {response}\")\n",
    "print(response)\n",
    "message = f\"Rate the quality of the response: {response}\"\n",
    "response = create_chat_completions(judge_model, message)\n",
    "print(f\"The response by {judge_model} to '{message}' is: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [1:19:40<00:00,  4.78s/it]\n",
      "100%|██████████| 1000/1000 [1:19:42<00:00,  4.78s/it]\n",
      " 50%|█████     | 2/4 [2:39:23<2:39:23, 4781.76s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def annotate_and_score_mwes(dataset, results_file, annotator_model, judge_model):\n",
    "    \"\"\"\n",
    "    Annotates and scores MWEs from the dataset using annotator and judge prompts.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset (dict): The dataset containing n-grams and their details.\n",
    "        results_file (str): Path to the file where results should be written.\n",
    "        annotator_message (str): Template for the annotator prompt.\n",
    "        judge_message (str): Template for the judge prompt.\n",
    "        annotator_model (str): Name of the model to use for annotating chat completions.\n",
    "        judge_model (str): Name of the model to use for judging chat completions.\n",
    "    \"\"\"\n",
    "    for ngram, data in tqdm(dataset.items()):\n",
    "        for phrase, details in tqdm(data.items()):\n",
    "            # Extract data for the prompt\n",
    "            n_gram = phrase\n",
    "            frequency = details.get(\"count\", \"N/A\")\n",
    "            pmi_score = details.get(\"PMI\", \"N/A\")\n",
    "            sentences = details.get(\"sentences\", [])\n",
    "\n",
    "            # Limit to three sentences for prompt context\n",
    "            sentence_1 = sentences[0] if len(sentences) > 0 else \"No example sentence available.\"\n",
    "            sentence_2 = sentences[1] if len(sentences) > 1 else \"No example sentence available.\"\n",
    "            sentence_3 = sentences[2] if len(sentences) > 2 else \"No example sentence available.\"\n",
    "\n",
    "            annotator_message = f\"\"\"You are an expert linguist helping to identify multi-word expressions (MWEs) in a large corpus. A multi-word expression is a sequence of words that form a single unit of meaning and cannot be easily deduced by the meanings of individual words.\n",
    "\n",
    "            Here is the information about a potential MWE:\n",
    "\n",
    "            **Candidate Phrase:** \"{n_gram}\"  \n",
    "            **PMI Score:** {pmi_score}  \n",
    "            **Frequency in Corpus:** {frequency}\n",
    "\n",
    "            ### Example Sentences:\n",
    "            1. \"{sentence_1}\"\n",
    "            2. \"{sentence_2}\"\n",
    "            3. \"{sentence_3}\"\n",
    "\n",
    "            **Questions:**\n",
    "            1. Does the candidate phrase overlap in meaning or structure with any known MWEs? If so, which one(s)?\n",
    "            2. Could this phrase be considered a new variation or extension of an existing MWE? Why?\n",
    "            3. If it is novel, does it demonstrate properties of an MWE such as idiomaticity or collocational fixedness?\n",
    "            4. How likely is it that this expression is becoming a trend in social media language? Rate this likelihood on a scale from 1-5.\n",
    "            5. Based on the given information, would you classify this candidate as:\n",
    "                - Novel MWE\n",
    "                - A variation of an existing MWE\n",
    "                - Not an MWE\n",
    "\n",
    "            Explain your decision with examples and reasoning.\n",
    "            \"\"\"\n",
    "\n",
    "\n",
    "            # Fill the annotator prompt\n",
    "            annotator_prompt = annotator_message.format(\n",
    "                n_gram=n_gram,\n",
    "                PMI_score=pmi_score,\n",
    "                frequency=frequency,\n",
    "                sentence_1=sentence_1,\n",
    "                sentence_2=sentence_2,\n",
    "                sentence_3=sentence_3\n",
    "            )\n",
    "\n",
    "            # Get the annotator's response\n",
    "            annotation = create_chat_completions(annotator_model, annotator_prompt)\n",
    "            if annotation is None:\n",
    "                annotation = \"Error: Annotation not available.\"\n",
    "            \n",
    "            \n",
    "            judge_message = f\"\"\"You are a judge evaluating the response of a linguist who has classified a multi-word expression (MWE) in a large corpus. Based on that, can you provide a label for the candidate phrase from the following options: [\"Novel MWE\", \"Variation of an existing MWE\", \"not an MWE\"]? Please provide only the label without any additional information.\"\"\"\n",
    "\n",
    "            # Fill the judge prompt\n",
    "            judge_prompt = judge_message.format(n_gram=n_gram)\n",
    "            \n",
    "            # Get the judge's response\n",
    "            judgment = create_chat_completions(judge_model, judge_prompt)\n",
    "            if judgment is None:\n",
    "                judgment = \"Error: Judgment not available.\"\n",
    "\n",
    "            # Prepare the result entry\n",
    "            result_entry = {\n",
    "                \"n_gram\": n_gram,\n",
    "                \"frequency\": frequency,\n",
    "                \"PMI_score\": pmi_score,\n",
    "                \"sentences\": [sentence_1, sentence_2, sentence_3],\n",
    "                \"annotation\": annotation,\n",
    "                \"judgment\": judgment\n",
    "            }\n",
    "\n",
    "            # Write the result to the file\n",
    "            with open(results_file, \"a\") as f:\n",
    "                f.write(json.dumps(result_entry) + \"\\n\")\n",
    "\n",
    "# Annotate and score the MWEs\n",
    "results_file = \"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences_results.json\"\n",
    "\n",
    "# test with a small subset of 10  n-grams per n = 2, 3, 4, 5\n",
    "# target = {k: dict(list(v.items())[:10]) for k, v in target.items()}\n",
    "\n",
    "# annotate_and_score_mwes\n",
    "annotate_and_score_mwes(target, results_file, annotator_model, judge_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the results\n",
    "results_file = \"../data/corpus/reddit_10/top_1000_ngrams_with_counts_pmi_sentences_results.json\"\n",
    "\n",
    "# show number of novel MWEs found\n",
    "with open(results_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    results = [json.loads(line) for line in f]\n",
    "\n",
    "# if results contain a \"novel MWE\" label string\n",
    "novel_mwes = [result for result in results if \"novel MWE\" in result[\"judgment\"]]\n",
    "print(f\"Number of novel MWEs found: {len(novel_mwes)}\")\n",
    "if len(novel_mwes) > 0:\n",
    "    print(f\"Example novel MWEs:\")\n",
    "    for result in novel_mwes:\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        print(f\"\\t\\t•Annotation: {result['annotation']}\")\n",
    "\n",
    "# if results contain a \"variation of an existing MWE\" label string\n",
    "variations = [result for result in results if \"variation of an existing MWE\" in result[\"judgment\"]]\n",
    "print(f\"Number of variations found: {len(variations)}\")\n",
    "if len(variations) > 0:\n",
    "    print(f\"Example variations:\")\n",
    "    for result in variations:\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        print(f\"\\t\\t•Annotation: {result['annotation']}\")\n",
    "\n",
    "# if results contain a \"not an MWE\" label string\n",
    "not_mwes = [result for result in results if \"not an MWE\" in result[\"judgment\"]]\n",
    "print(f\"Number of non-MWEs found: {len(not_mwes)}\")\n",
    "if len(not_mwes) > 0:\n",
    "    print(f\"Example non-MWEs:\")\n",
    "    for result in not_mwes:\n",
    "        print(f\"\\t•{result['n_gram']}\")\n",
    "        print(f\"\\t\\t•Annotation: {result['annotation']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
