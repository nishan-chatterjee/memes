{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting total lines in the file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 74004228/74004228 [05:31<00:00, 223557.61line/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting complete. Files saved in bookcorpus/chunks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def split_jsonl_file(input_file, output_dir, num_chunks=10):\n",
    "    \"\"\"\n",
    "    Splits a large JSONL file into multiple smaller JSON files in a memory-efficient manner.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input JSONL file.\n",
    "        output_dir (str): Directory to store the output chunk files.\n",
    "        num_chunks (int): Number of chunks to split the file into.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # First, determine the total number of lines in the file\n",
    "    print(\"Counting total lines in the file...\")\n",
    "    total_lines = sum(1 for _ in open(input_file, 'r', encoding='utf-8'))\n",
    "\n",
    "    # Calculate the approximate number of lines per chunk\n",
    "    lines_per_chunk = total_lines // num_chunks\n",
    "\n",
    "    # Initialize variables for splitting\n",
    "    chunk_index = 1\n",
    "    current_chunk_lines = []\n",
    "    chunk_file_path = os.path.join(output_dir, f\"{chunk_index}.json\")\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        with tqdm(total=total_lines, desc=\"Processing\", unit=\"line\") as pbar:\n",
    "            for line_number, line in enumerate(infile, start=1):\n",
    "                # Parse the JSON line\n",
    "                data = json.loads(line)\n",
    "\n",
    "                # Add the parsed data to the current chunk lines\n",
    "                current_chunk_lines.append(data)\n",
    "\n",
    "                # If we've reached the lines per chunk, write the chunk to a file\n",
    "                if len(current_chunk_lines) >= lines_per_chunk and chunk_index < num_chunks:\n",
    "                    with open(chunk_file_path, 'w', encoding='utf-8') as chunk_file:\n",
    "                        json.dump(current_chunk_lines, chunk_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "                    # Clear the current chunk and move to the next one\n",
    "                    current_chunk_lines = []\n",
    "                    chunk_index += 1\n",
    "                    chunk_file_path = os.path.join(output_dir, f\"{chunk_index}.json\")\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "            # Write any remaining lines to the last chunk file\n",
    "            if current_chunk_lines:\n",
    "                with open(chunk_file_path, 'w', encoding='utf-8') as chunk_file:\n",
    "                    json.dump(current_chunk_lines, chunk_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Splitting complete. Files saved in {output_dir}\")\n",
    "\n",
    "# Example usage\n",
    "split_jsonl_file(\"bookcorpus/bookcorpus.jsonl\", \"bookcorpus/chunks\", num_chunks=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "\n",
    "def generate_ngrams(text, n):\n",
    "    \"\"\"Generate n-grams from a given text.\"\"\"\n",
    "    words = text.split()\n",
    "    return [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
    "\n",
    "def count_ngrams_in_chunks(input_dir, output_file, n_values=[2, 3, 4, 5]):\n",
    "    \"\"\"\n",
    "    Generate and count n-gram pairs for a corpus stored in chunks.\n",
    "\n",
    "    Args:\n",
    "        input_dir (str): Directory containing the chunk files.\n",
    "        output_file (str): Path to the output JSON file to store n-gram counts.\n",
    "        n_values (list): List of n values for which to generate n-grams.\n",
    "    \"\"\"\n",
    "    # Dictionary to store n-gram counts for each n\n",
    "    ngram_counts = {n: defaultdict(int) for n in n_values}\n",
    "\n",
    "    # Get the list of chunk files\n",
    "    chunk_files = sorted(\n",
    "        [os.path.join(input_dir, file) for file in os.listdir(input_dir) if file.endswith('.json')]\n",
    "    )\n",
    "\n",
    "    # Outer progress bar for chunks\n",
    "    with tqdm(total=len(chunk_files), desc=\"Chunks\", unit=\"chunk\") as chunk_pbar:\n",
    "        # Process each chunk file\n",
    "        for chunk_file in chunk_files:\n",
    "            with open(chunk_file, 'r', encoding='utf-8') as infile:\n",
    "                data = json.load(infile)\n",
    "\n",
    "                # Inner progress bar for lines in the current chunk\n",
    "                with tqdm(total=len(data), desc=f\"Lines in {os.path.basename(chunk_file)}\", unit=\"line\", leave=False) as line_pbar:\n",
    "                    # Iterate over each entry in the chunk\n",
    "                    for entry in data:\n",
    "                        text = entry.get(\"text\", \"\")\n",
    "\n",
    "                        # Generate and count n-grams for each n\n",
    "                        for n in n_values:\n",
    "                            ngrams = generate_ngrams(text, n)\n",
    "                            for ngram in ngrams:\n",
    "                                ngram_counts[n][ngram] += 1\n",
    "\n",
    "                        # Update inner progress bar\n",
    "                        line_pbar.update(1)\n",
    "\n",
    "            # Update outer progress bar\n",
    "            chunk_pbar.update(1)\n",
    "\n",
    "    # Convert defaultdicts to regular dictionaries for JSON serialization\n",
    "    ngram_counts_serializable = {\n",
    "        n: {\" \".join(ngram): count for ngram, count in ngram_counts[n].items()}\n",
    "        for n in n_values\n",
    "    }\n",
    "\n",
    "    # Write the n-gram counts to the output JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(ngram_counts_serializable, outfile, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"N-gram counts saved to {output_file}\")\n",
    "\n",
    "# Example usage\n",
    "count_ngrams_in_chunks(\"bookcorpus/chunks\", \"ngram_counts.json\", n_values=[2, 3, 4, 5])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
